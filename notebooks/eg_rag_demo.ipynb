{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EG-RAG: Evidence Graph based Retrieval-Augmented Generation\n",
    "\n",
    "This notebook demonstrates the EG-RAG pipeline for handling conflicting information in multi-document QA.\n",
    "\n",
    "**Paper**: AAMAS 2026 - \"EG-RAG: Retrieval-Augmented Generation with Evidence Graph for Reliable Multi-Document Reasoning\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, install the required dependencies and set up your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "# !pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key is set (do NOT hardcode your key!)\n",
    "assert os.getenv('OPENAI_API_KEY'), \"Please set OPENAI_API_KEY in your .env file\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import openai\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'MPS' if torch.backends.mps.is_available() else 'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core Functions\n",
    "\n",
    "### 3.1 Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_sent_tokenize(text: str) -> list[str]:\n",
    "    \"\"\"Split text into sentences based on punctuation.\"\"\"\n",
    "    return re.split(r'(?<=[.\\?\\!])\\s+', text.strip())\n",
    "\n",
    "\n",
    "def separate_passages(document: str) -> list[str]:\n",
    "    \"\"\"Split document by 'Document:' delimiter.\"\"\"\n",
    "    raw_passages = document.split(\"Document:\")\n",
    "    return [p.strip() for p in raw_passages if p.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Key Sentence Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \\\n",
    "         torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "def extract_key_sentences(\n",
    "    docs: list[str],\n",
    "    query: str,\n",
    "    model_name: str = \"all-MiniLM-L6-v2\",\n",
    "    top_k: int = 3\n",
    ") -> tuple[list[str], list[float]]:\n",
    "    \"\"\"\n",
    "    Extract top-k key sentences from documents based on semantic similarity to query.\n",
    "    \n",
    "    Args:\n",
    "        docs: List of document passages\n",
    "        query: User query\n",
    "        model_name: SentenceTransformer model name\n",
    "        top_k: Number of top sentences to return\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (sentences, scores)\n",
    "    \"\"\"\n",
    "    # Tokenize documents into sentences\n",
    "    sentences = []\n",
    "    for doc in docs:\n",
    "        sentences.extend(simple_sent_tokenize(doc))\n",
    "    \n",
    "    if not sentences:\n",
    "        return [], []\n",
    "    \n",
    "    # Load embedding model\n",
    "    embedder = SentenceTransformer(model_name, device=device)\n",
    "    \n",
    "    # Encode sentences and query\n",
    "    sent_embs = embedder.encode(sentences, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    query_emb = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    scores = np.dot(sent_embs, query_emb)\n",
    "    \n",
    "    # Get top-k indices\n",
    "    top_k = min(top_k, len(sentences))\n",
    "    top_idx = np.argsort(scores)[-top_k:][::-1]\n",
    "    \n",
    "    return [sentences[i] for i in top_idx], [float(scores[i]) for i in top_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 NLI Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NLI model\n",
    "nli_model_name = \"roberta-large-mnli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(nli_model_name)\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name).to(device).eval()\n",
    "\n",
    "# Label mapping: {0: contradiction, 1: neutral, 2: entailment}\n",
    "idx2label = {0: \"contradiction\", 1: \"neutral\", 2: \"support\"}\n",
    "\n",
    "\n",
    "def nli_classify(sentence_a: str, sentence_b: str) -> tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Classify the relationship between two sentences using NLI.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (label, probability)\n",
    "        label: 'contradiction', 'neutral', or 'support'\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(sentence_a, sentence_b, return_tensors=\"pt\", \n",
    "                       truncation=True, padding=True, max_length=512).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = nli_model(**inputs).logits\n",
    "        probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "    \n",
    "    idx = int(np.argmax(probs))\n",
    "    return idx2label[idx], float(probs[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Evidence Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_evidence_graph(sentences: list[str], scores: list[float]) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Build an evidence graph from key sentences.\n",
    "    \n",
    "    Nodes: Each sentence with attributes (text, score)\n",
    "    Edges: NLI relationships with attributes (label, weight)\n",
    "           weight = nli_probability * score_i * score_j\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes\n",
    "    for i, (sent, score) in enumerate(zip(sentences, scores)):\n",
    "        G.add_node(i, text=sent, score=score)\n",
    "    \n",
    "    # Add edges based on NLI classification\n",
    "    n = len(sentences)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            label, prob = nli_classify(sentences[i], sentences[j])\n",
    "            weight = prob * scores[i] * scores[j]\n",
    "            G.add_edge(i, j, label=label, weight=weight)\n",
    "    \n",
    "    return G\n",
    "\n",
    "\n",
    "def find_subgraphs_by_label(G: nx.Graph, label: str) -> list[nx.Graph]:\n",
    "    \"\"\"Find connected subgraphs containing only edges with the specified label.\"\"\"\n",
    "    H = nx.Graph((u, v, d) for u, v, d in G.edges(data=True) if d[\"label\"] == label)\n",
    "    return [G.subgraph(c).copy() for c in nx.connected_components(H) if len(c) > 1]\n",
    "\n",
    "\n",
    "def extract_clusters(G: nx.Graph, subgraphs: list[nx.Graph]) -> list[list[str]]:\n",
    "    \"\"\"Extract sentence clusters from subgraphs.\"\"\"\n",
    "    return [[G.nodes[node][\"text\"] for node in subg.nodes()] for subg in subgraphs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 LLM Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(\n",
    "    question: str,\n",
    "    documents: list[str],\n",
    "    formatted_key_sents: list[str],\n",
    "    contradictory_clusters: list[list[str]],\n",
    "    neutral_clusters: list[list[str]],\n",
    "    support_clusters: list[list[str]]\n",
    ") -> list[dict]:\n",
    "    \"\"\"Create a QA prompt with evidence graph analysis.\"\"\"\n",
    "    prompt = f\"\"\"You are an expert retrieval-based QA system.\n",
    "\n",
    "Follow these steps for each input:\n",
    "\n",
    "You will receive a question together with a set of key passages.\n",
    "Each key passage includes a document number, a confidence score, and a key sentence.\n",
    "\n",
    "Focusing on the highest-scoring sentence in each document, consult all of the key sentences to determine the correct answer step-by-step.\n",
    "\n",
    "To find the answer, focus on the highest-scoring sentences in each document in key passages, and refer to the clusters.\n",
    "It's important to find answers by comparing the key sentences extracted from each document.\n",
    "\n",
    "* ANSWER EXTRACTION RULES\n",
    "- Extract the *exact* noun phrase(s) or term(s) that answer the question.\n",
    "- No explanations, no punctuation, no extra text - just the terms.\n",
    "\n",
    "Reference Information:\n",
    "\n",
    "Question: {question}\n",
    "Documents: {documents}\n",
    "Key passages: {formatted_key_sents}\n",
    "Contradictory clusters: {contradictory_clusters}\n",
    "Neutral clusters: {neutral_clusters}\n",
    "Support clusters: {support_clusters}\n",
    "\n",
    "You must provide the final answer as follows:\n",
    "[\"answer1\", \"answer2\"]\n",
    "\"\"\"\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful and logical assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "\n",
    "def generate_answer(messages: list[dict], model: str = \"gpt-4o-mini\") -> str:\n",
    "    \"\"\"Generate answer using OpenAI API.\"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Main EG-RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eg_rag_pipeline(\n",
    "    context: str,\n",
    "    question: str,\n",
    "    top_k: int = 3,\n",
    "    verbose: bool = True\n",
    ") -> tuple[str, nx.Graph]:\n",
    "    \"\"\"\n",
    "    Run the EG-RAG pipeline.\n",
    "    \n",
    "    Args:\n",
    "        context: Document context (may contain multiple 'Document:' sections)\n",
    "        question: User question\n",
    "        top_k: Number of key sentences per document\n",
    "        verbose: Print intermediate results\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (answer_string, evidence_graph)\n",
    "    \"\"\"\n",
    "    # 1. Separate passages\n",
    "    passages = separate_passages(context)\n",
    "    \n",
    "    # 2. Extract key sentences from each passage\n",
    "    key_sents = []\n",
    "    key_scores = []\n",
    "    key_idxs = []\n",
    "    \n",
    "    for idx, passage in enumerate(passages):\n",
    "        sents, scores = extract_key_sentences([passage], question, top_k=top_k)\n",
    "        key_sents.extend(sents)\n",
    "        key_scores.extend(scores)\n",
    "        key_idxs.extend([idx] * len(sents))\n",
    "    \n",
    "    # Format key sentences\n",
    "    formatted_key_sents = [\n",
    "        f\"[{idx}] ({score:.3f}) {sent}\"\n",
    "        for idx, sent, score in zip(key_idxs, key_sents, key_scores)\n",
    "    ]\n",
    "    \n",
    "    # 3. Build evidence graph\n",
    "    G = build_evidence_graph(key_sents, key_scores)\n",
    "    \n",
    "    # 4. Find clusters\n",
    "    contradiction_subgraphs = find_subgraphs_by_label(G, \"contradiction\")\n",
    "    neutral_subgraphs = find_subgraphs_by_label(G, \"neutral\")\n",
    "    support_subgraphs = find_subgraphs_by_label(G, \"support\")\n",
    "    \n",
    "    contradictory_clusters = extract_clusters(G, contradiction_subgraphs)\n",
    "    neutral_clusters = extract_clusters(G, neutral_subgraphs)\n",
    "    support_clusters = extract_clusters(G, support_subgraphs)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Key Sentences:\")\n",
    "        print(\"\\n\".join(formatted_key_sents))\n",
    "        \n",
    "        if contradictory_clusters:\n",
    "            print(\"\\nContradictory clusters found:\")\n",
    "            for i, cluster in enumerate(contradictory_clusters, 1):\n",
    "                print(f\"  Cluster {i}:\")\n",
    "                for s in cluster:\n",
    "                    print(f\"   - {s}\")\n",
    "        else:\n",
    "            print(\"\\nNo contradictory clusters found.\")\n",
    "    \n",
    "    # 5. Generate answer\n",
    "    messages = create_prompt(\n",
    "        question, passages, formatted_key_sents,\n",
    "        contradictory_clusters, neutral_clusters, support_clusters\n",
    "    )\n",
    "    answer = generate_answer(messages)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nAnswer: {answer}\")\n",
    "    \n",
    "    return answer, G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Demo: Single Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with conflicting information\n",
    "example_context = \"\"\"\n",
    "Document: Anti-inflammatory drugs are often used to control the effects of inflammation. \n",
    "Glucocorticoids are the most powerful of these drugs; however, these drugs can have many \n",
    "undesirable side effects, such as central obesity, hyperglycemia, osteoporosis. \n",
    "Immunosuppressive drugs such as cyclosporin prevent T cells from responding to signals \n",
    "correctly by inhibiting signal transduction pathways.\n",
    "\n",
    "Document: Anti-inflammatory drugs are often used to control the effects of inflammation. \n",
    "Glucocorticoids are the most powerful of these drugs; however, these drugs can have many \n",
    "undesirable side effects, such as central obesity, hyperglycemia, osteoporosis. \n",
    "Immunosuppressive drugs such as aspirin prevent T cells from responding to signals \n",
    "correctly by inhibiting signal transduction pathways.\n",
    "\"\"\"\n",
    "\n",
    "example_question = \"What is an example of an immunosuppressive drug that prevents T cell activity by altering signal transduction pathways?\"\n",
    "\n",
    "print(f\"Question: {example_question}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "answer, graph = eg_rag_pipeline(example_context, example_question, top_k=3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate on FaithEval-Inconsistent Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FaithEval-Inconsistent dataset\n",
    "dataset = load_dataset(\"Salesforce/FaithEval-inconsistent-v1.0\", split=\"test\")\n",
    "print(f\"Dataset size: {len(dataset)} samples\")\n",
    "\n",
    "# Limit samples for demo (set to None for full evaluation)\n",
    "num_samples = 10\n",
    "if num_samples:\n",
    "    dataset = dataset.select(range(num_samples))\n",
    "    print(f\"Using {num_samples} samples for demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "\n",
    "for i, example in enumerate(tqdm(dataset, desc=\"Evaluating\")):\n",
    "    answer, _ = eg_rag_pipeline(\n",
    "        example['context'],\n",
    "        example['question'],\n",
    "        top_k=3,\n",
    "        verbose=False\n",
    "    )\n",
    "    predictions.append(answer)\n",
    "    ground_truths.append(example['answers'])\n",
    "    \n",
    "    tqdm.write(f\"[{i}] Predicted: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "\n",
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Lowercase, strip, and remove punctuation.\"\"\"\n",
    "    return re.sub(r'[^\\w\\s]', '', s.lower()).strip()\n",
    "\n",
    "\n",
    "def count_matches(true_list: list[str], pred_list: list[str]) -> int:\n",
    "    \"\"\"Count matched answers using substring matching.\"\"\"\n",
    "    norm_true = [normalize_answer(t) for t in true_list]\n",
    "    norm_pred = [normalize_answer(p) for p in pred_list]\n",
    "    matched = set()\n",
    "    \n",
    "    for i, t in enumerate(norm_true):\n",
    "        for p in norm_pred:\n",
    "            if p and (p in t or t in p):\n",
    "                matched.add(i)\n",
    "                break\n",
    "    return len(matched)\n",
    "\n",
    "\n",
    "def parse_prediction(pred_str: str) -> list[str]:\n",
    "    \"\"\"Parse prediction string to list.\"\"\"\n",
    "    pred_str = (pred_str or '').strip()\n",
    "    if pred_str.lower() == 'unknown':\n",
    "        return []\n",
    "    try:\n",
    "        return json.loads(pred_str)\n",
    "    except:\n",
    "        try:\n",
    "            return ast.literal_eval(pred_str)\n",
    "        except:\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "total_matched = 0\n",
    "total_true = 0\n",
    "fully_correct = 0\n",
    "\n",
    "for i, (pred, true_list) in enumerate(zip(predictions, ground_truths)):\n",
    "    pred_list = parse_prediction(pred)\n",
    "    matched = count_matches(true_list, pred_list)\n",
    "    \n",
    "    n_true = len(true_list)\n",
    "    if matched == n_true:\n",
    "        fully_correct += 1\n",
    "    \n",
    "    total_matched += matched\n",
    "    total_true += n_true\n",
    "    \n",
    "    acc = matched / n_true if n_true > 0 else 0\n",
    "    print(f\"Item {i}: {acc:.2f} ({matched}/{n_true} matched)\")\n",
    "\n",
    "# Summary\n",
    "element_accuracy = total_matched / total_true if total_true > 0 else 0\n",
    "item_accuracy = fully_correct / len(predictions) if predictions else 0\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Element-level Accuracy: {element_accuracy:.2%} ({total_matched}/{total_true})\")\n",
    "print(f\"Item-level Accuracy (EM): {item_accuracy:.2%} ({fully_correct}/{len(predictions)})\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Evidence Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_graph(G: nx.Graph, title: str = \"Evidence Graph\"):\n",
    "    \"\"\"Visualize the evidence graph.\"\"\"\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=500, node_color='lightblue')\n",
    "    nx.draw_networkx_labels(G, pos)\n",
    "    \n",
    "    # Draw edges with colors based on label\n",
    "    edge_colors = {\n",
    "        'contradiction': 'red',\n",
    "        'neutral': 'gray',\n",
    "        'support': 'green'\n",
    "    }\n",
    "    \n",
    "    for label, color in edge_colors.items():\n",
    "        edges = [(u, v) for u, v, d in G.edges(data=True) if d['label'] == label]\n",
    "        if edges:\n",
    "            nx.draw_networkx_edges(G, pos, edgelist=edges, edge_color=color, \n",
    "                                   width=2, alpha=0.7, label=label)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize the graph from our example\n",
    "visualize_graph(graph, \"Evidence Graph - Immunosuppressive Drug Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Experiment with Different top_k Values\n",
    "\n",
    "The `top_k` parameter controls how many key sentences are extracted per document:\n",
    "- `top_k=1`: Fastest, minimal context\n",
    "- `top_k=2`: Balanced\n",
    "- `top_k=3`: Default, good coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different top_k values\n",
    "for k in [1, 2, 3]:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"top_k = {k}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    answer, _ = eg_rag_pipeline(example_context, example_question, top_k=k, verbose=False)\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Citation\n",
    "\n",
    "If you find this code useful, please cite our paper:\n",
    "\n",
    "```bibtex\n",
    "@inproceedings{\n",
    "hong2026egrag,\n",
    "title={{EG}-{RAG}: Retrieval-Augmented Generation with Evidence Graph for Reliable Multi-Document Reasoning},\n",
    "author={Seunggwan Hong and Junhyung Moon and Eunkyeong Lee and Jaehyoung Park and Hyunseung Choo},\n",
    "booktitle={The 25th International Conference on Autonomous Agents and Multi-Agent Systems},\n",
    "year={2026},\n",
    "url={https://openreview.net/forum?id=ahosApE8Ap}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
